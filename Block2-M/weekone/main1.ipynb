{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21b86f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\anaconda3\\envs\\torchgpu\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\anaconda3\\envs\\torchgpu\\lib\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\anaconda3\\envs\\torchgpu\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\anaconda3\\envs\\torchgpu\\lib\\site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\envs\\torchgpu\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\envs\\torchgpu\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97525db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72508267",
   "metadata": {},
   "source": [
    "<code>\n",
    "Tokenization\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b255357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Hello!', 'How are you doing today?', \"I hope you're doing fine.\"],\n",
       " ['Hello',\n",
       "  '!',\n",
       "  'How',\n",
       "  'are',\n",
       "  'you',\n",
       "  'doing',\n",
       "  'today',\n",
       "  '?',\n",
       "  'I',\n",
       "  'hope',\n",
       "  'you',\n",
       "  \"'re\",\n",
       "  'doing',\n",
       "  'fine',\n",
       "  '.'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hello! How are you doing today? I hope you're doing fine.\"\n",
    "sentence_token = nltk.sent_tokenize(sentence)\n",
    "word_token = nltk.word_tokenize(sentence)\n",
    "\n",
    "sentence_token , word_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3015f6",
   "metadata": {},
   "source": [
    "<code>stopwords</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8705d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', '!', 'How', 'today', '?', 'I', 'hope', \"'re\", 'fine', '.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = stopwords.words(\"english\")\n",
    "filtered_sentence = [ word for word in word_token if word not in english_stopwords]\n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79749c79",
   "metadata": {},
   "source": [
    "<code>stemming</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45832874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running -> run\n",
      "jumped -> jump\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "words = [\"Running\" , \"jumped\"]\n",
    "\n",
    "for word in words : \n",
    "    stemmed_word = ps.stem(word)\n",
    "    print(f\"{word} -> {stemmed_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f860b91",
   "metadata": {},
   "source": [
    "<code>Lemmatization vs Stemming</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d14a24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'ve\", 'used', 'the', 'computer', 'to', 'get', 'information']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', \"'ve\", 'use', 'the', 'comput', 'to', 'get', 'inform']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer , PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"I've used the computer to get informations\"\n",
    "\n",
    "\n",
    "\n",
    "filtered_sentence = [ lemmatizer.lemmatize(word) for word in word_tokenize(sentence)]\n",
    "print(filtered_sentence)\n",
    "\n",
    "filtered_sentence = [ ps.stem(word) for word in word_tokenize(sentence)]\n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3e790d",
   "metadata": {},
   "source": [
    "<code>Regular Expression</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e86d0fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "txt = \"\"\"\n",
    "Elon musk's phone number is 9991116666, call him if you have any questions on dodgecoin\n",
    "Tesla's revenue is 40 billion\n",
    "Tesla's CFO number (999)-333-7777\n",
    "\"\"\"\n",
    "pattern = r'\\(\\d{3}\\)-\\d{3}-\\d{4}|\\d{10}'\n",
    "cleaned = re.findall(pattern , txt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "31f974e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Note 1 - Overview', 'Note 2 - Summary of Significant Accounting Policies']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"\"\"\n",
    "\n",
    "Note 1 - Overview\n",
    "Note 2 - Summary of Significant Accounting Policies\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "pattern = r'Note \\d - [^\\n]*'\n",
    "cleaned = re.findall(pattern , txt)\n",
    "\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d9ac4b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FY2021 Q1', 'fy2020 Q4', 'fy2030 q1']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt =\"\"\" \n",
    "    The gross cost of operating lease vehicles in FY2021 Q1 was $4.85 billion\n",
    "    In previous quarter i.e. fy2020 Q4 it was $3 billion fy2030 q1\n",
    "\"\"\"\n",
    "\n",
    "pattern = r'fy\\d{4} Q[1-4]'\n",
    "cleaned = re.findall(pattern , txt , flags=re.IGNORECASE)\n",
    "\n",
    "cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "42c90719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://twitter.com/elonmusk',\n",
       " 'https://twitter.com/teslarati',\n",
       " 'https://twitter.com/dummy_tesla',\n",
       " 'https://twitter.com/dummy_2_tesla']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = '''\n",
    "Follow our leader Elon musk on twitter here: https://twitter.com/elonmusk\n",
    ", more information \n",
    "on Tesla's products can be found at https://www.tesla.com/. Also here are leading influencers \n",
    "for tesla related news,\n",
    "https://twitter.com/teslarati\n",
    "https://twitter.com/dummy_tesla\n",
    "https://twitter.com/dummy_2_tesla\n",
    "'''\n",
    "z = r'https:\\/\\/twitter.com\\/\\w[^\\n]*' # todo: type your regex here\n",
    "\n",
    "re.findall(z, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f334caac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c3238",
   "metadata": {},
   "source": [
    "<code>POS - part of speech</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9433f0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mashary', 'JJ'), ('loves', 'NNS'), ('coding', 'VBG'), ('!', '.')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "txt = word_tokenize(\"Mashary loves coding!\")\n",
    "\n",
    "result = pos_tag(txt)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd3d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ea173725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     token    pos pos_explanation\n",
      "0  Mashary  PROPN     proper noun\n",
      "1    Loves   VERB            verb\n",
      "2   Coding  PROPN     proper noun\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from tabulate import tabulate\n",
    "# Load your spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Assuming 'result' is a spaCy Doc object\n",
    "data = []\n",
    "for token in result:\n",
    "    data.append({\n",
    "        'token': token.text,\n",
    "        'pos': token.pos_,\n",
    "        'pos_explanation': spacy.explain(token.pos_)\n",
    "    })\n",
    "\n",
    "# Create a single DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
