{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ebd9db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
    "import re\n",
    "import torch as t \n",
    "from torch.utils.data import TensorDataset , DataLoader \n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import classification_report , confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f807821f",
   "metadata": {},
   "source": [
    "Raw Text ↓ Data Split ↓ Cleaning ↓ TF-IDF (fit on train only) ↓ Transform val/test ↓ Dataset & DataLoader ↓ MLP (with Dropout) ↓ Loss (BCE or CrossEntropy) ↓ Optimizer (Adam) ↓ Training Loop ↓ Validation ↓ Evaluation ↓ Save model + vectorizer ↓ Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0489f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self , \n",
    "                split_data:bool , \n",
    "                feature_extraction:str,\n",
    "                data):\n",
    "        \n",
    "        self.split_data = split_data\n",
    "        self.feature_extraction = feature_extraction\n",
    "        self.data = data\n",
    "        self.model = None \n",
    "        self.vectorizer = None\n",
    "    \n",
    "    def clean_data(self):\n",
    "            \"\"\"\n",
    "            _summary_\n",
    "            Clean dataset : tokenized,stopwords,lemmatization\n",
    "            Returns:\n",
    "                X , Y\n",
    "                _type_: Dataframe(text,label) \n",
    "            \"\"\"\n",
    "            df = pd.read_csv(self.data)\n",
    "            tokenized = []\n",
    "            Lematizer = WordNetLemmatizer()\n",
    "            stop_words = stopwords.words(\"english\")\n",
    "            text = []\n",
    "            for sentence in df[\"text\"]:\n",
    "                t_sentence = sent_tokenize(sentence)\n",
    "                tokenized.append(t_sentence)\n",
    "                for word in t_sentence :\n",
    "                    review = word.lower()\n",
    "                    review = review.split()\n",
    "                    review = [Lematizer.lemmatize(word) for word in review if word not in stop_words]\n",
    "                    text.append(review)\n",
    "            \n",
    "                                \n",
    "            new_df = pd.DataFrame({\n",
    "                \"text\": text,\n",
    "                \"label\": df[\"label\"]\n",
    "            })\n",
    "            X = new_df[\"text\"]\n",
    "            Y = new_df[\"label\"].squeeze() \n",
    "            return X , Y\n",
    "            \n",
    "    def split_clean_data(self):\n",
    "        \"\"\"_summary_\n",
    "        Check split_data(bool)\n",
    "        True : split_data\n",
    "        False : do not split_data\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        assert len(self.data) != 0 \n",
    "    \n",
    "        if self.split_data:\n",
    "            X , Y = self.clean_data()\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "                X, Y, test_size=0.2, random_state=42\n",
    "            )\n",
    "            return X_train, X_test, Y_train, Y_test\n",
    "        else: \n",
    "            X , Y = self.clean_data()\n",
    "            return X , Y\n",
    "        \n",
    "    def set_vectorizer(self):\n",
    "        assert len(self.feature_extraction) >= 3 \n",
    "                    \n",
    "        if len(self.split_clean_data()) == 4 :\n",
    "            X_train, X_test, Y_train, Y_test = self.split_clean_data()\n",
    "                        \n",
    "            X_train = [\" \".join(doc) if isinstance(doc, list) else doc for doc in X_train]\n",
    "            X_test = [\" \".join(doc) if isinstance(doc, list) else doc for doc in X_test]\n",
    "            \n",
    "            Y_train = [\" \".join(doc) if isinstance(doc, list) else doc for doc in Y_train]\n",
    "            Y_test = [\" \".join(doc) if isinstance(doc, list) else doc for doc in Y_test]\n",
    "            \n",
    "        else :\n",
    "            X , Y = self.split_clean_data()\n",
    "            X = [\" \".join(doc) if isinstance(doc, list) else doc for doc in X]\n",
    "            Y = [\" \".join(doc) if isinstance(doc, list) else doc for doc in Y]\n",
    "        \n",
    "        if self.feature_extraction == \"tfidf\":\n",
    "            self.vectorizer = TfidfVectorizer()\n",
    "            X_train_vectorized = self.vectorizer.fit_transform(X_train)\n",
    "            X_test_vectorized = self.vectorizer.transform(X_test)\n",
    "            return X_train_vectorized, X_test_vectorized, Y_train, Y_test\n",
    "                \n",
    "        if self.feature_extraction == \"bow\":\n",
    "            self.vectorizer = CountVectorizer()\n",
    "            \n",
    "            if self.split_data:  # if you have train/test split\n",
    "                X_train_vectorized = self.vectorizer.fit_transform(X_train)\n",
    "                X_test_vectorized = self.vectorizer.transform(X_test)\n",
    "                return X_train_vectorized, X_test_vectorized, Y_train, Y_test\n",
    "            else:  # no split\n",
    "                X_vectorized = self.vectorizer.fit_transform(X)\n",
    "                return X_vectorized, Y\n",
    "        \n",
    "    def utilities(self):\n",
    "        if len(self.set_vectorizer()) == 4 :\n",
    "            X_train_vectorized , X_test_vectorized , Y_train , Y_test = self.set_vectorizer()\n",
    "            \n",
    "            \n",
    "            X_train_vectorized = t.tensor(\n",
    "            X_train_vectorized.toarray(),\n",
    "            dtype=t.float32\n",
    "            )\n",
    "            Y_train = t.LongTensor(Y_train)\n",
    "            Y_test = t.LongTensor(Y_test)\n",
    "            \n",
    "            X_test_vectorized = t.tensor(\n",
    "            X_test_vectorized.toarray(),\n",
    "            dtype=t.float32\n",
    "            )\n",
    "            \n",
    "            train_dataset = TensorDataset(X_train_vectorized , Y_train)\n",
    "            test_dataset =  TensorDataset(X_test_vectorized , Y_test)\n",
    "            train_data_loader = DataLoader(train_dataset , 64 , shuffle=True)\n",
    "            test_data_loader = DataLoader(test_dataset , 64 , shuffle=False)\n",
    "            \n",
    "            return train_data_loader , test_data_loader \n",
    "        \n",
    "        else : \n",
    "            X_vectorized , Y_vectorized = self.set_vectorizer()\n",
    "            X_vectorized = t.tensor(\n",
    "            X_vectorized.toarray(),\n",
    "            dtype=t.float32\n",
    "            )    \n",
    "            Y_vectorized = t.LongTensor(Y_vectorized)\n",
    "            train_dataset = TensorDataset(X_vectorized )\n",
    "            test_dataset =  TensorDataset(Y_vectorized )\n",
    "            train_data_loader = DataLoader(train_dataset , 64 , shuffle=True)\n",
    "            test_data_loader = DataLoader(test_dataset , 64 , shuffle=False)\n",
    "            \n",
    "            return train_data_loader , test_data_loader\n",
    "    \n",
    "    def create_model(self , in_features , out_features):\n",
    "        class Model(t.nn.Module):\n",
    "            def __init__(self, in_features , out_features):\n",
    "                super().__init__()\n",
    "                self.network = t.nn.Sequential(\n",
    "                    t.nn.Linear(in_features , 256),\n",
    "                    t.nn.ReLU(),\n",
    "                    t.nn.Dropout(0.3),\n",
    "                    t.nn.Linear(256 , 128),\n",
    "                    t.nn.ReLU(),\n",
    "                    t.nn.Dropout(0.3),\n",
    "                    t.nn.Linear(128 , out_features),\n",
    "                )\n",
    "            \n",
    "            def forward(self , x):\n",
    "                return self.network(x)\n",
    "    \n",
    "        return Model(in_features, out_features)\n",
    "    \n",
    "    def train(self , epochs , batch_size ,lr=0.0001):\n",
    "        train_data_loader , test_data_loader = self.utilities()\n",
    "        device = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        \n",
    "        # Get one batch to determine input size\n",
    "        sample_batch = next(iter(train_data_loader))\n",
    "        input_size = sample_batch[0].shape[1]\n",
    "        num_classes = len(t.unique(sample_batch[1]))\n",
    "\n",
    "        self.model = self.create_model(input_size, num_classes).to(device)\n",
    "                \n",
    "\n",
    "        \n",
    "        c = t.nn.CrossEntropyLoss()\n",
    "        optim = t.optim.Adam(self.model.parameters() , lr=lr)\n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for idx , (batch_x , batch_y) in enumerate(train_data_loader):\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "\n",
    "                y_pred = self.model(batch_x)\n",
    "                loss = c(y_pred , batch_y)\n",
    "\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                \n",
    "\n",
    "                all_preds.append(y_pred.argmax(dim=1).cpu())\n",
    "                all_labels.append(batch_y.cpu())\n",
    "\n",
    "                if idx % 50 == 0 :\n",
    "                    print(f\"Epoch {epoch}, Batch {idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "        all_preds = t.cat(all_preds)\n",
    "        all_labels = t.cat(all_labels)\n",
    "\n",
    "        cr = classification_report(all_labels.numpy(), all_preds.numpy())\n",
    "        cm = confusion_matrix(all_labels.numpy(), all_preds.numpy())\n",
    "        print(cr)\n",
    "        print(cm)\n",
    "        return self.model\n",
    "    \n",
    "    def predict(self, text):\n",
    "        # Preprocessing: same as training\n",
    "        stop_words = stopwords.words(\"english\")\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        review = text.lower().split()\n",
    "        final = [lemmatizer.lemmatize(word) for word in review if word not in stop_words]\n",
    "        clean_text = \" \".join(final)\n",
    "\n",
    "        # Use the trained vectorizer from training\n",
    "        X = self.vectorizer.transform([clean_text]).toarray()\n",
    "\n",
    "        # Convert to tensor and send to the correct device\n",
    "        device = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "        X_tensor = t.FloatTensor(X).to(device)\n",
    "\n",
    "        # Make prediction\n",
    "        self.model.eval()\n",
    "        with t.no_grad():\n",
    "            output = self.model(X_tensor)\n",
    "            pred = output.argmax(dim=1).item()\n",
    "            probabilities = t.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "        return pred, probabilities.cpu().numpy()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5b6581f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 1.7808\n",
      "Epoch 0, Batch 50, Loss: 1.7627\n",
      "Epoch 0, Batch 100, Loss: 1.6749\n",
      "Epoch 0, Batch 150, Loss: 1.6013\n",
      "Epoch 1, Batch 0, Loss: 1.6382\n",
      "Epoch 1, Batch 50, Loss: 1.4477\n",
      "Epoch 1, Batch 100, Loss: 1.4465\n",
      "Epoch 1, Batch 150, Loss: 1.3553\n",
      "Epoch 2, Batch 0, Loss: 1.3182\n",
      "Epoch 2, Batch 50, Loss: 1.1685\n",
      "Epoch 2, Batch 100, Loss: 1.1184\n",
      "Epoch 2, Batch 150, Loss: 0.9720\n",
      "Epoch 3, Batch 0, Loss: 0.9371\n",
      "Epoch 3, Batch 50, Loss: 0.7701\n",
      "Epoch 3, Batch 100, Loss: 0.6983\n",
      "Epoch 3, Batch 150, Loss: 0.7922\n",
      "Epoch 4, Batch 0, Loss: 0.5517\n",
      "Epoch 4, Batch 50, Loss: 0.4081\n",
      "Epoch 4, Batch 100, Loss: 0.6807\n",
      "Epoch 4, Batch 150, Loss: 0.4911\n",
      "Epoch 5, Batch 0, Loss: 0.5453\n",
      "Epoch 5, Batch 50, Loss: 0.5897\n",
      "Epoch 5, Batch 100, Loss: 0.4313\n",
      "Epoch 5, Batch 150, Loss: 0.3138\n",
      "Epoch 6, Batch 0, Loss: 0.3395\n",
      "Epoch 6, Batch 50, Loss: 0.3537\n",
      "Epoch 6, Batch 100, Loss: 0.2584\n",
      "Epoch 6, Batch 150, Loss: 0.3109\n",
      "Epoch 7, Batch 0, Loss: 0.2437\n",
      "Epoch 7, Batch 50, Loss: 0.2417\n",
      "Epoch 7, Batch 100, Loss: 0.2269\n",
      "Epoch 7, Batch 150, Loss: 0.2582\n",
      "Epoch 8, Batch 0, Loss: 0.1993\n",
      "Epoch 8, Batch 50, Loss: 0.2973\n",
      "Epoch 8, Batch 100, Loss: 0.1644\n",
      "Epoch 8, Batch 150, Loss: 0.1876\n",
      "Epoch 9, Batch 0, Loss: 0.0777\n",
      "Epoch 9, Batch 50, Loss: 0.1080\n",
      "Epoch 9, Batch 100, Loss: 0.2436\n",
      "Epoch 9, Batch 150, Loss: 0.1475\n",
      "Epoch 10, Batch 0, Loss: 0.1165\n",
      "Epoch 10, Batch 50, Loss: 0.1826\n",
      "Epoch 10, Batch 100, Loss: 0.1382\n",
      "Epoch 10, Batch 150, Loss: 0.0424\n",
      "Epoch 11, Batch 0, Loss: 0.1038\n",
      "Epoch 11, Batch 50, Loss: 0.1331\n",
      "Epoch 11, Batch 100, Loss: 0.0745\n",
      "Epoch 11, Batch 150, Loss: 0.1541\n",
      "Epoch 12, Batch 0, Loss: 0.0856\n",
      "Epoch 12, Batch 50, Loss: 0.0543\n",
      "Epoch 12, Batch 100, Loss: 0.1450\n",
      "Epoch 12, Batch 150, Loss: 0.0992\n",
      "Epoch 13, Batch 0, Loss: 0.0673\n",
      "Epoch 13, Batch 50, Loss: 0.0577\n",
      "Epoch 13, Batch 100, Loss: 0.0544\n",
      "Epoch 13, Batch 150, Loss: 0.0476\n",
      "Epoch 14, Batch 0, Loss: 0.0986\n",
      "Epoch 14, Batch 50, Loss: 0.0943\n",
      "Epoch 14, Batch 100, Loss: 0.1401\n",
      "Epoch 14, Batch 150, Loss: 0.0355\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.91      0.88     55800\n",
      "           1       0.80      0.95      0.87     65115\n",
      "           2       0.90      0.61      0.72     15120\n",
      "           3       0.91      0.75      0.82     25980\n",
      "           4       0.77      0.71      0.74     23100\n",
      "           5       0.98      0.26      0.42      6885\n",
      "\n",
      "    accuracy                           0.83    192000\n",
      "   macro avg       0.87      0.70      0.74    192000\n",
      "weighted avg       0.84      0.83      0.82    192000\n",
      "\n",
      "[[50943  4163     9   218   466     1]\n",
      " [ 2255 61960   374    40   478     8]\n",
      " [ 1020  3717  9149   220  1013     1]\n",
      " [ 2983  3149    18 19441   389     0]\n",
      " [ 2235  3098    71  1167 16494    35]\n",
      " [  590  1069   520   174  2716  1816]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " array([[0.5246639 , 0.40335864, 0.00270367, 0.05045561, 0.01544334,\n",
       "         0.00337479]], dtype=float32))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip = Pipeline(True , \"bow\" , \"training.csv\")\n",
    "pip.train(15 , 64 )\n",
    "\n",
    "\n",
    "sentence = \"I feel tired\"\n",
    "\n",
    "pip.predict(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
