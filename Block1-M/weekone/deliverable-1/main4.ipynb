{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dbaf1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training neural network...\n",
      "Epoch    0 | Loss: 1.238938\n",
      "Epoch  500 | Loss: 0.096696\n",
      "Epoch 1000 | Loss: 0.087517\n",
      "Epoch 1500 | Loss: 0.079298\n",
      "Epoch 2000 | Loss: 0.077099\n",
      "Epoch 2500 | Loss: 0.067734\n",
      "Epoch 3000 | Loss: 0.064467\n",
      "Epoch 3500 | Loss: 0.059829\n",
      "Epoch 4000 | Loss: 0.059309\n",
      "Epoch 4500 | Loss: 0.053939\n",
      "Epoch 5000 | Loss: 0.054247\n",
      "Epoch 5500 | Loss: 0.047026\n",
      "Epoch 6000 | Loss: 0.046568\n",
      "Epoch 6500 | Loss: 0.041211\n",
      "Epoch 7000 | Loss: 0.044144\n",
      "Epoch 7500 | Loss: 0.038206\n",
      "Epoch 8000 | Loss: 0.039139\n",
      "Epoch 8500 | Loss: 0.036215\n",
      "Epoch 9000 | Loss: 0.037867\n",
      "Epoch 9500 | Loss: 0.033726\n",
      "Epoch 10000 | Loss: 0.035133\n",
      "Epoch 10500 | Loss: 0.031867\n",
      "Epoch 11000 | Loss: 0.032637\n",
      "Epoch 11500 | Loss: 0.029712\n",
      "Epoch 12000 | Loss: 0.030072\n",
      "Epoch 12500 | Loss: 0.027597\n",
      "Epoch 13000 | Loss: 0.027874\n",
      "Epoch 13500 | Loss: 0.025790\n",
      "Epoch 14000 | Loss: 0.026055\n",
      "Epoch 14500 | Loss: 0.024386\n",
      "Epoch 15000 | Loss: 0.024029\n",
      "Epoch 15500 | Loss: 0.022843\n",
      "Epoch 16000 | Loss: 0.022127\n",
      "Epoch 16500 | Loss: 0.021452\n",
      "Epoch 17000 | Loss: 0.020820\n",
      "Epoch 17500 | Loss: 0.020282\n",
      "Epoch 18000 | Loss: 0.019574\n",
      "Epoch 18500 | Loss: 0.019051\n",
      "Epoch 19000 | Loss: 0.018567\n",
      "Epoch 19500 | Loss: 0.018114\n",
      "Epoch 20000 | Loss: 0.016621\n",
      "Epoch 20500 | Loss: 0.015968\n",
      "Epoch 21000 | Loss: 0.015555\n",
      "Epoch 21500 | Loss: 0.015230\n",
      "Epoch 22000 | Loss: 0.014888\n",
      "Epoch 22500 | Loss: 0.014636\n",
      "Epoch 23000 | Loss: 0.014376\n",
      "Epoch 23500 | Loss: 0.014153\n",
      "Epoch 24000 | Loss: 0.013887\n",
      "Epoch 24500 | Loss: 0.013629\n",
      "Epoch 25000 | Loss: 0.013400\n",
      "Epoch 25500 | Loss: 0.013226\n",
      "Epoch 26000 | Loss: 0.013062\n",
      "Epoch 26500 | Loss: 0.012917\n",
      "Epoch 27000 | Loss: 0.012783\n",
      "Epoch 27500 | Loss: 0.012662\n",
      "Epoch 28000 | Loss: 0.012545\n",
      "Epoch 28500 | Loss: 0.012421\n",
      "Epoch 29000 | Loss: 0.012318\n",
      "Epoch 29500 | Loss: 0.012231\n",
      "Epoch 30000 | Loss: 0.012155\n",
      "Epoch 30500 | Loss: 0.012076\n",
      "Epoch 31000 | Loss: 0.012011\n",
      "Epoch 31500 | Loss: 0.011949\n",
      "Epoch 32000 | Loss: 0.011882\n",
      "Epoch 32500 | Loss: 0.011831\n",
      "Epoch 33000 | Loss: 0.011780\n",
      "Epoch 33500 | Loss: 0.011733\n",
      "Epoch 34000 | Loss: 0.011688\n",
      "Epoch 34500 | Loss: 0.011647\n",
      "Epoch 35000 | Loss: 0.011611\n",
      "Epoch 35500 | Loss: 0.011573\n",
      "Epoch 36000 | Loss: 0.011537\n",
      "Epoch 36500 | Loss: 0.011505\n",
      "Epoch 37000 | Loss: 0.011476\n",
      "Epoch 37500 | Loss: 0.011448\n",
      "Epoch 38000 | Loss: 0.011424\n",
      "Epoch 38500 | Loss: 0.011399\n",
      "Epoch 39000 | Loss: 0.011375\n",
      "Epoch 39500 | Loss: 0.011354\n",
      "Epoch 40000 | Loss: 0.011335\n",
      "Epoch 40500 | Loss: 0.011315\n",
      "Epoch 41000 | Loss: 0.011296\n",
      "Epoch 41500 | Loss: 0.011281\n",
      "Epoch 42000 | Loss: 0.011264\n",
      "Epoch 42500 | Loss: 0.011248\n",
      "Epoch 43000 | Loss: 0.011234\n",
      "Epoch 43500 | Loss: 0.011221\n",
      "Epoch 44000 | Loss: 0.011207\n",
      "Epoch 44500 | Loss: 0.011195\n",
      "Epoch 45000 | Loss: 0.011184\n",
      "Epoch 45500 | Loss: 0.011174\n",
      "Epoch 46000 | Loss: 0.011163\n",
      "Epoch 46500 | Loss: 0.011154\n",
      "Epoch 47000 | Loss: 0.011146\n",
      "Epoch 47500 | Loss: 0.011137\n",
      "Epoch 48000 | Loss: 0.011129\n",
      "Epoch 48500 | Loss: 0.011121\n",
      "Epoch 49000 | Loss: 0.011114\n",
      "Epoch 49500 | Loss: 0.011108\n",
      "Epoch 50000 | Loss: 0.011101\n",
      "Epoch 50500 | Loss: 0.011095\n",
      "Epoch 51000 | Loss: 0.011089\n",
      "Epoch 51500 | Loss: 0.011084\n",
      "Epoch 52000 | Loss: 0.011079\n",
      "Epoch 52500 | Loss: 0.011074\n",
      "Epoch 53000 | Loss: 0.011069\n",
      "Epoch 53500 | Loss: 0.011065\n",
      "Epoch 54000 | Loss: 0.011061\n",
      "Epoch 54500 | Loss: 0.011057\n",
      "Epoch 55000 | Loss: 0.011053\n",
      "Epoch 55500 | Loss: 0.011050\n",
      "Epoch 56000 | Loss: 0.011046\n",
      "Epoch 56500 | Loss: 0.011044\n",
      "Epoch 57000 | Loss: 0.011040\n",
      "Epoch 57500 | Loss: 0.011038\n",
      "Epoch 58000 | Loss: 0.011035\n",
      "Epoch 58500 | Loss: 0.011033\n",
      "Epoch 59000 | Loss: 0.011030\n",
      "Epoch 59500 | Loss: 0.011028\n",
      "Epoch 60000 | Loss: 0.011026\n",
      "Epoch 60500 | Loss: 0.011024\n",
      "Epoch 61000 | Loss: 0.011022\n",
      "Epoch 61500 | Loss: 0.011020\n",
      "Epoch 62000 | Loss: 0.011018\n",
      "Epoch 62500 | Loss: 0.011017\n",
      "Epoch 63000 | Loss: 0.011015\n",
      "Epoch 63500 | Loss: 0.011014\n",
      "Epoch 64000 | Loss: 0.011012\n",
      "Epoch 64500 | Loss: 0.011011\n",
      "Epoch 65000 | Loss: 0.011010\n",
      "Epoch 65500 | Loss: 0.011009\n",
      "Epoch 66000 | Loss: 0.011008\n",
      "Epoch 66500 | Loss: 0.011006\n",
      "Epoch 67000 | Loss: 0.011005\n",
      "Epoch 67500 | Loss: 0.011004\n",
      "Epoch 68000 | Loss: 0.011004\n",
      "Epoch 68500 | Loss: 0.011003\n",
      "Epoch 69000 | Loss: 0.011002\n",
      "Epoch 69500 | Loss: 0.011001\n",
      "Epoch 70000 | Loss: 0.011000\n",
      "Epoch 70500 | Loss: 0.011000\n",
      "Epoch 71000 | Loss: 0.010999\n",
      "Epoch 71500 | Loss: 0.010998\n",
      "Epoch 72000 | Loss: 0.010998\n",
      "Epoch 72500 | Loss: 0.010997\n",
      "Epoch 73000 | Loss: 0.010997\n",
      "Epoch 73500 | Loss: 0.010996\n",
      "Epoch 74000 | Loss: 0.010996\n",
      "Epoch 74500 | Loss: 0.010995\n",
      "Epoch 75000 | Loss: 0.010995\n",
      "Epoch 75500 | Loss: 0.010994\n",
      "Epoch 76000 | Loss: 0.010994\n",
      "Epoch 76500 | Loss: 0.010994\n",
      "Epoch 77000 | Loss: 0.010993\n",
      "Epoch 77500 | Loss: 0.010993\n",
      "Epoch 78000 | Loss: 0.010993\n",
      "Epoch 78500 | Loss: 0.010992\n",
      "Epoch 79000 | Loss: 0.010992\n",
      "Epoch 79500 | Loss: 0.010992\n",
      "Epoch 80000 | Loss: 0.010992\n",
      "Epoch 80500 | Loss: 0.010991\n",
      "Epoch 81000 | Loss: 0.010991\n",
      "Epoch 81500 | Loss: 0.010991\n",
      "Epoch 82000 | Loss: 0.010991\n",
      "Epoch 82500 | Loss: 0.010991\n",
      "Epoch 83000 | Loss: 0.010990\n",
      "Epoch 83500 | Loss: 0.010990\n",
      "Epoch 84000 | Loss: 0.010990\n",
      "Epoch 84500 | Loss: 0.010990\n",
      "Epoch 85000 | Loss: 0.010990\n",
      "Epoch 85500 | Loss: 0.010990\n",
      "Epoch 86000 | Loss: 0.010989\n",
      "Epoch 86500 | Loss: 0.010989\n",
      "Epoch 87000 | Loss: 0.010989\n",
      "Epoch 87500 | Loss: 0.010989\n",
      "Epoch 88000 | Loss: 0.010989\n",
      "Epoch 88500 | Loss: 0.010989\n",
      "Epoch 89000 | Loss: 0.010989\n",
      "Epoch 89500 | Loss: 0.010989\n",
      "\n",
      "=== Model Performance on Test Set ===\n",
      "MAE:  $15,897.79\n",
      "RMSE: $20,704.78\n",
      "R²:   0.2727\n",
      "\n",
      "=== Prediction for Sample House ===\n",
      "Features: SqFt=2250, Bed=4, Bath=3, Offers=3, Brick=1\n",
      "Predicted price: $181,689\n",
      "\n",
      "=== Feature Importance ===\n",
      "Price      | Importance: 0.5345 | Correlation with Price: 0.5536\n",
      "SqFt       | Importance: 0.3503 | Correlation with Price: 0.5020\n",
      "Bedrooms   | Importance: 0.2315 | Correlation with Price: 0.5010\n",
      "Bathrooms  | Importance: 0.3107 | Correlation with Price: -0.3758\n",
      "Offers     | Importance: 0.2533 | Correlation with Price: 0.4851\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ===== Load and prepare data =====\n",
    "def load_and_prepare_data(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    data.drop(columns=[\"Neighborhood\", \"Home\"], inplace=True)\n",
    "    data[\"Brick\"] = data[\"Brick\"].map({\"No\": 0, \"Yes\": 1})\n",
    "    \n",
    "    X = data.drop(columns=\"Price\").values.astype(float)\n",
    "    y = data[\"Price\"].values.reshape(-1, 1).astype(float)\n",
    "    \n",
    "    return X, y, data.columns.tolist()\n",
    "\n",
    "# ===== Scale data =====\n",
    "def scale_data(X_train, X_test, y_train):\n",
    "    # Scale features\n",
    "    X_mean = X_train.mean(axis=0)\n",
    "    X_std = X_train.std(axis=0)\n",
    "    X_std[X_std == 0] = 1\n",
    "    X_train_s = (X_train - X_mean) / X_std\n",
    "    X_test_s = (X_test - X_mean) / X_std\n",
    "    \n",
    "    # Scale target\n",
    "    y_mean = y_train.mean()\n",
    "    y_std = y_train.std()\n",
    "    y_train_s = (y_train - y_mean) / y_std\n",
    "    \n",
    "    return X_train_s, X_test_s, y_train_s, X_mean, X_std, y_mean, y_std\n",
    "\n",
    "# ===== Neural Network with ReLU =====\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden1=32, hidden2=16, output_size=1, learning_rate=0.01):\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Initialize weights with Xavier initialization\n",
    "        self.w1 = np.random.randn(input_size, hidden1) * np.sqrt(2.0 / (input_size + hidden1))\n",
    "        self.b1 = np.zeros((1, hidden1))\n",
    "        self.w2 = np.random.randn(hidden1, hidden2) * np.sqrt(2.0 / (hidden1 + hidden2))\n",
    "        self.b2 = np.zeros((1, hidden2))\n",
    "        self.w3 = np.random.randn(hidden2, output_size) * np.sqrt(2.0 / (hidden2 + output_size))\n",
    "        self.b3 = np.zeros((1, output_size))\n",
    "        \n",
    "        self.lr = learning_rate\n",
    "        self.best_weights = None\n",
    "        self.best_loss = float('inf')\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_deriv(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.Z1 = X @ self.w1 + self.b1\n",
    "        self.A1 = self.relu(self.Z1)\n",
    "        self.Z2 = self.A1 @ self.w2 + self.b2\n",
    "        self.A2 = self.relu(self.Z2)\n",
    "        self.Z3 = self.A2 @ self.w3 + self.b3\n",
    "        return self.Z3\n",
    "    \n",
    "    def backward(self, X, y, y_pred):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dZ3 = 2 * (y_pred - y) / m\n",
    "        dW3 = self.A2.T @ dZ3\n",
    "        db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
    "        \n",
    "        # Hidden layer 2 gradients\n",
    "        dA2 = dZ3 @ self.w3.T\n",
    "        dZ2 = dA2 * self.relu_deriv(self.Z2)\n",
    "        dW2 = self.A1.T @ dZ2\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Hidden layer 1 gradients\n",
    "        dA1 = dZ2 @ self.w2.T\n",
    "        dZ1 = dA1 * self.relu_deriv(self.Z1)\n",
    "        dW1 = X.T @ dZ1\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "        \n",
    "        return dW1, db1, dW2, db2, dW3, db3\n",
    "    \n",
    "    def update_weights(self, dW1, db1, dW2, db2, dW3, db3):\n",
    "        self.w1 -= self.lr * dW1\n",
    "        self.b1 -= self.lr * db1\n",
    "        self.w2 -= self.lr * dW2\n",
    "        self.b2 -= self.lr * db2\n",
    "        self.w3 -= self.lr * dW3\n",
    "        self.b3 -= self.lr * db3\n",
    "    \n",
    "    def train(self, X_train, y_train, epochs=5000, verbose=True):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X_train)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = np.mean((y_pred - y_train)**2)\n",
    "            \n",
    "            # Save best weights\n",
    "            if loss < self.best_loss:\n",
    "                self.best_loss = loss\n",
    "                self.best_weights = (\n",
    "                    self.w1.copy(), self.b1.copy(),\n",
    "                    self.w2.copy(), self.b2.copy(),\n",
    "                    self.w3.copy(), self.b3.copy()\n",
    "                )\n",
    "            \n",
    "            # Backward pass\n",
    "            dW1, db1, dW2, db2, dW3, db3 = self.backward(X_train, y_train, y_pred)\n",
    "            \n",
    "            # Update weights\n",
    "            self.update_weights(dW1, db1, dW2, db2, dW3, db3)\n",
    "            \n",
    "            # Learning rate decay\n",
    "            if epoch % 1000 == 0 and epoch > 0:\n",
    "                self.lr *= 0.9\n",
    "            \n",
    "            if verbose and epoch % 500 == 0:\n",
    "                print(f\"Epoch {epoch:4d} | Loss: {loss:.6f}\")\n",
    "        \n",
    "        # Restore best weights\n",
    "        if self.best_weights:\n",
    "            self.w1, self.b1, self.w2, self.b2, self.w3, self.b3 = self.best_weights\n",
    "    \n",
    "    def predict(self, X, y_mean, y_std):\n",
    "        predictions = self.forward(X)\n",
    "        return predictions * y_std + y_mean\n",
    "\n",
    "# ===== Main execution =====\n",
    "def main():\n",
    "    # Load data\n",
    "    X, y, feature_names = load_and_prepare_data(\"house-prices.csv\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Scale data\n",
    "    X_train_s, X_test_s, y_train_s, X_mean, X_std, y_mean, y_std = scale_data(\n",
    "        X_train, X_test, y_train\n",
    "    )\n",
    "    \n",
    "    # Create and train neural network\n",
    "    print(\"Training neural network...\")\n",
    "    nn = NeuralNetwork(\n",
    "        input_size=X_train_s.shape[1],\n",
    "        hidden1=32,\n",
    "        hidden2=16,\n",
    "        learning_rate=0.1\n",
    "    )\n",
    "    \n",
    "    nn.train(X_train_s, y_train_s, epochs=90000, verbose=True)\n",
    "    \n",
    "    # Make predictions on test set\n",
    "    y_pred = nn.predict(X_test_s, y_mean, y_std)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n=== Model Performance on Test Set ===\")\n",
    "    print(f\"MAE:  ${mae:,.2f}\")\n",
    "    print(f\"RMSE: ${rmse:,.2f}\")\n",
    "    print(f\"R²:   {r2:.4f}\")\n",
    "    \n",
    "    # Predict your specific sample\n",
    "    sample = np.array([[2250, 4, 3, 3, 1]])  # From Home #1\n",
    "    sample_s = (sample - X_mean) / X_std\n",
    "    sample_pred = nn.predict(sample_s, y_mean, y_std)\n",
    "    \n",
    "    print(f\"\\n=== Prediction for Sample House ===\")\n",
    "    print(f\"Features: SqFt={sample[0,0]}, Bed={sample[0,1]}, Bath={sample[0,2]}, \"\n",
    "          f\"Offers={sample[0,3]}, Brick={sample[0,4]}\")\n",
    "    print(f\"Predicted price: ${sample_pred[0,0]:,.0f}\")\n",
    "\n",
    "    # Feature importance analysis\n",
    "    print(f\"\\n=== Feature Importance ===\")\n",
    "    feature_importance = np.mean(np.abs(nn.w1), axis=1)\n",
    "    for i, (feature, importance) in enumerate(zip(feature_names[:-1], feature_importance)):\n",
    "        print(f\"{feature:10} | Importance: {importance:.4f} | Correlation with Price: {np.corrcoef(X_train[:, i], y_train.flatten())[0, 1]:.4f}\")\n",
    "    \n",
    "    return nn, X_mean, X_std, y_mean, y_std\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nn, X_mean, X_std, y_mean, y_std = main()\n",
    "    \n",
    "    # You can now use the trained model for new predictions:\n",
    "    # new_house = np.array([[sqft, bedrooms, bathrooms, offers, brick]])\n",
    "    # new_house_scaled = (new_house - X_mean) / X_std\n",
    "    # predicted_price = nn.predict(new_house_scaled, y_mean, y_std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
